# mTCP: A Highly Scalable User-level TCP Stack for Multicore Systems

## 0x00 论文简介

在多核系统上，当存在大量的小型消息事务时，操作系统无法充分利用CPU的性能，存在性能瓶颈。针对该问题，作者提出了mTCP-一个应用于多核系统的高性能用户级TCP堆栈，经过实验测试，该TCP堆栈可以将系统处理小型消息事物的性能提高25倍之多。

## 0x01 主要内容

#### 问题：

在当前TCP连接中，大部分的消息包都是小尺寸的。在蜂窝网络中，超过90%的TCP包都小于32kb，超过50%的包小于4KB。

而当前的Linux Kernel架构在处理小封包的性能存在很大瓶颈，比如：

1. Lack of connection locality

   缺乏连接局部性。在多线程通信应用中，我们往往共享同一个监听套接字。而这种运行模式回导致，多个线程会互相竞争使用套接字的接受队列，导致性能的下降。 此外，执行处理TCP连接的内核代码的核心可能与运行实际发送和接收数据的应用程序代码的核心不同。 由于CPU缓存未命中和缓存线共享增加，这种连接局部性的缺乏会引起额外的开销。

2. Shared ﬁle descriptor space:

   共享文件描述符空间：在符合POSIX的操作系统中，文件描述符（fd）空间在进程中共享。而在处理大量并发连接的繁忙服务器中，多线程之间的竞争锁会带来性能开销。 反过来，套接字的文件描述符的使用还为通过Linux虚拟文件系统（VFS）创造了额外的开销，这是一个用于支持常见文件操作的伪文件系统层。 

3. Inefﬁcient per-packet processing

   不利于每个数据包的处理：目前，操作系统会对每一个包进行单独处理。然而，这种处理模式会带来更多的开销。

4. System call overhead

   系统调用的开销：在大量小封包并发连接到来时，传统的套接字需要频繁的用户/内核模式切换。频繁的系统调用会污染处理器状态（比如高速缓存、分支预测表），从而导致性能下降。另外，用户/内核模式的切换也会导致一定的开销。

#### 已有的解决方案：

1. 修改kernel的，如MegaPipe，FlexSC
2. 在用户空间提供高速的Packet I / O，通常是直接跟网卡操作，跳过内核，如netmap，DPDK，PSIO。

#### 已有方案的不足：

1. 修改内核代码太繁琐，麻烦，无法很好应用到真正的工业服务器上
2. 用户空间库的缺点
3. 没有统一的用户接口，导致开发者需要处理很多额外的工作，开发困难，且跨平台不稳定

#### MTCP解决思路：

1. 不修改kernel，只修改用户空间库。
2. 为每个核维护自己的数据结构，包括tcb、socket的buffer以及相应队列，从而避免竞争锁问题，并增加缓存性能
3. 为了保证多网卡下的高效处理，mTCP采用PacketShader I / O引擎（PSIO）来提供高效率的事件驱动的数据包I / O接口，提供了ps_select。
4. 避免频繁用户/内核切换的开销，mTcp采用一个名为零线程的TCP来提供功能，以用户态的模式来进行交互。
5. 高效的TCP timer管理，并进行优先级管理，对小封包连接进行优化，保证性能
6. 批处理网络报文，使用batch，来平摊额外开销。

整体设计实现如下：

![1](pic/1.png)

## 0x02 学习总结

整篇文章结构很棒，令读者可以很快地抓到细节。该文章详细地讲述了当前内核在面临大量并发TCP连接过程中会遇到的性能问题及一些解决方案，为我补充了这一方面的空白。

另外，这篇文章对其的设计与实现（还有源码），讲的很详细，学习到了很多。



## 0x03 参考

1. https://www.usenix.org/node/179774
2. https://www.usenix.org/conference/atc14/technical-sessions/presentation/mtcp-highly-scalable-user-level-tcp-stack-multicore
3. http://shader.kaist.edu/mtcp/